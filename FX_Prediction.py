# -*- coding: utf-8 -*-
"""완성본 23일 예측 형식 (cycle_35).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UmzeZ7BkjE_bn7tuuYXk-Ngr_fKemdgj
"""

import tensorflow as tf
import numpy as np
import pandas as pd
import datetime
import matplotlib.pyplot as plt

# 랜덤에 의해 똑같은 결과를 재현하도록 시드 설정
tf.set_random_seed(777)

# 데이터 전처리
def data_standardization(x):
    x_np = np.asarray(x)
    return (x_np - x_np.mean()) / x_np.std()

# 데이터 전처리
# 너무 작거나 너무 큰 값은 학습에 방해되므로 정규화
# x가 양수라는 가정하에 최소값과 최대값을 이용하여 0~1사이의 값으로 변환
# 0으로 나누는 오류 방지차원에서 분자에 1e-7 더해줌
def min_max_scaling(x):
    x_np = np.asarray(x)
    return (x_np - x_np.min()) / (x_np.max() - x_np.min() + 1e-7)

# 데이터 전처리
# 역정규화_정규화 이전의 값으로 되돌림
def reverse_min_max_scaling(org_x, x):
    org_x_np = np.asarray(org_x)
    x_np = np.asarray(x)
    return (x_np * (org_x_np.max() - org_x_np.min() + 1e-7)) + org_x_np.min()

input_data_column_cnt = 13  # 입력데이터의 컬럼 개수(Variable 개수)
output_data_column_cnt = 1 # 결과데이터의 컬럼 개수
 
seq_length = 35            # 1개 시퀀스의 길이(시계열데이터 입력 개수)
rnn_cell_hidden_dim = 20   # 각 셀의 (hidden)출력 크기
forget_bias = 1.0          # 망각편향(기본값 1.0)
num_stacked_layers = 4     # stacked LSTM layers 개수
keep_prob = 1.0            # dropout할 때 keep할 비율
 
epoch_num = 1000           # 에폭 횟수(학습용전체데이터를 몇 회 반복해서 학습할 것인가 입력)
learning_rate = 0.01       # 학습률

currency_file_name = 'JPY_KRW_08_23.csv' # 환율 데이타셋
encoding = 'euc-kr' # 문자 인코딩
names = ['date','EUR-OPEN','EUR-HIGH','EUR-LOW', 'USD-OPEN','USD-HIGH','USD-LOW','GBP-OPEN','GBP-HIGH','GBP-LOW','JPY-OPEN','JPY-HIGH','JPY-LOW', 'JPY']
raw_dataframe = pd.read_csv(currency_file_name, names=names, encoding=encoding) #판다스이용 csv파일 로딩
raw_dataframe.info() # 데이터 정보 출력

del raw_dataframe['date'] # 시간열을 제거하고 dataframe 재생성하지 않기
 
currency_info = raw_dataframe.values[1:].astype(np.float) # 
print("currency_info.shape: ", currency_info.shape)
print("currency_info[0]: ", currency_info[0])

# 환율 정보_1 (OPEN-HIGH-LOW)
parameter_1 = currency_info[:, :3]
norm_parameter_1 = min_max_scaling(parameter_1) 
print("parameter_1.shape: ", parameter_1.shape)
print("parameter_1[0]: ", parameter_1[0])
print("norm_parameter_1[0]: ", norm_parameter_1[0])
print("="*100) 

# 환율 정보_2 (OPEN-HIGH-LOW)
parameter_2 = currency_info[:, 3:6]
norm_parameter_2 = min_max_scaling(parameter_2) 
print("parameter_2.shape: ", parameter_2.shape)
print("parameter_2[0]: ", parameter_2[0])
print("norm_parameter_2[0]: ", norm_parameter_2[0])
print("="*100)

# 환율 정보_3 (OPEN-HIGH-LOW)
parameter_3 = currency_info[:, 6:9]
norm_parameter_3 = min_max_scaling(parameter_3) 
print("parameter_3.shape: ", parameter_3.shape)
print("parameter_3[0]: ", parameter_3[0])
print("norm_parameter_3[0]: ", norm_parameter_3[0])
print("="*100)

# 타겟 환율 정보_4 (OPEN-HIGH-LOW)
target_price = currency_info[:, 9:-1]
norm_target_price = min_max_scaling(target_price) 
print("target_price.shape: ", target_price.shape)
print("target_price[0]: ", target_price[0])
print("norm_target_price[0]: ", norm_target_price[0])
print("="*100)

# 타겟 (CLOSE 환율의 종가_ 다음날 오픈가)
result = currency_info[:,-1:]
norm_result = min_max_scaling(result) 
print("result.shape: ", result.shape)
print("result[0]: ", result[0])
print("norm_result[0]: ", norm_result[0])
print("="*100)

x = np.concatenate((norm_parameter_1, norm_parameter_2, norm_parameter_3, norm_target_price, norm_result), axis=1) # axis=1, 세로로 합친다
print("x.shape: ", x.shape)
print("x[0]: ", x[0])    # x의 첫 값
print("x[-1]: ", x[-1])  # x의 마지막 값
print("="*100) 
 
y = x[:, [-1]]

print("y[0]: ",y[0])     # y의 첫 값
print("y[-1]: ",y[-1])   # y의 마지막 값

dataX = [] # 입력으로 사용될 Sequence Data
dataY = [] # 출력(타켓)으로 사용
 
for i in range(0, len(y) - seq_length):
    _x = x[i : i+seq_length]
    _y = y[i + seq_length] # 다음 나타날 환율
    if i is 0:
        print(_x, "->", _y) # 첫번째 행만 출력
    dataX.append(_x) # dataX 리스트에 추가
    dataY.append(_y) # dataY 리스트에 추가

# 전체 데이타셋 중 70% 훈련용 데이터로 사용 
train_size = int(len(dataY) * 0.70) 

# 나머지(30%)를 테스트용 데이터로 사용 
test_size = len(dataY) - train_size 

# 데이터를 잘라 학습용 데이터 생성
trainX = np.array(dataX[0:train_size])
trainY = np.array(dataY[0:train_size])
 
# 데이터를 잘라 테스트용 데이터 생성
testX = np.array(dataX[train_size:len(dataX)])
testY = np.array(dataY[train_size:len(dataY)])

X = tf.placeholder(tf.float32, [None, seq_length, input_data_column_cnt])
print("X: ", X)
Y = tf.placeholder(tf.float32, [None, 1])
print("Y: ", Y)
 
# 검증용 측정지표를 산출하기 위한 targets, predictions를 생성
targets = tf.placeholder(tf.float32, [None, 1])
print("targets: ", targets)
 
predictions = tf.placeholder(tf.float32, [None, 1])
print("predictions: ", predictions)

def lstm_cell():
    # LSTM셀을 생성
    # num_units: 각 Cell 출력 크기
    # forget_bias:  to the biases of the forget gate 
    #              (default: 1)  in order to reduce the scale of forgetting in the beginning of the training.
    # state_is_tuple: True ==> accepted and returned states are 2-tuples of the c_state and m_state.
    # state_is_tuple: False ==> they are concatenated along the column axis.
    cell = tf.contrib.rnn.BasicLSTMCell(num_units=rnn_cell_hidden_dim, 
                                        forget_bias=forget_bias, state_is_tuple=True, activation=tf.nn.softsign)
    if keep_prob < 1.0:
        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)
    return cell

#num_stacked_layers개의 층으로 쌓인 Stacked RNNs 생성
stackedRNNs = [lstm_cell() for _ in range(num_stacked_layers)]
multi_cells = tf.contrib.rnn.MultiRNNCell(stackedRNNs, state_is_tuple=True) if num_stacked_layers > 1 else lstm_cell()

#LSTM셀들을 연결
hypothesis, _states = tf.nn.dynamic_rnn(multi_cells, X, dtype=tf.float32)
print("hypothesis: ", hypothesis)
 
# LSTM RNN의 마지막 (hidden)출력만을 사용 => hypothesis[:, -1]
# 과거의 환율 데이터셋을 이용해서 다음날의 환율 1개를 예측 => MANY-TO-ONE형태
hypothesis = tf.contrib.layers.fully_connected(hypothesis[:, -1], output_data_column_cnt, activation_fn=tf.identity)

# 손실함수_ 평균제곱오차
loss = tf.reduce_sum(tf.square(hypothesis - Y))
# 최적화함수_ AdamOptimizer
optimizer = tf.train.AdamOptimizer(learning_rate)
 
train = optimizer.minimize(loss)
 
# RMSE(Root Mean Square Error)
rmse = tf.sqrt(tf.reduce_mean(tf.squared_difference(targets, predictions)))

train_error_summary = [] # 학습용 데이터 오류 기록
test_error_summary = []  # 테스트용 데이터 오류 기록
test_predict = ''        # 테스트용데이터로 예측한 결과
 
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# 학습
start_time = datetime.datetime.now() # 시작시간을 기록
print('학습을 시작합니다...')
for epoch in range(epoch_num):
    _, _loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})
    if ((epoch+1) % 100 == 0) or (epoch == epoch_num-1): # 100번째마다 또는 마지막 epoch인 경우
        # rmse오차 (학습용데이터)
        train_predict = sess.run(hypothesis, feed_dict={X: trainX})
        train_error = sess.run(rmse, feed_dict={targets: trainY, predictions: train_predict})
        train_error_summary.append(train_error)
 
        # rmse오차 (테스트용데이터)
        test_predict = sess.run(hypothesis, feed_dict={X: testX})
        test_error = sess.run(rmse, feed_dict={targets: testY, predictions: test_predict})
        test_error_summary.append(test_error)
        
        # 현재 오류 출력
        print("epoch: {}, train_error(A): {}, test_error(B): {}, B-A: {}".format(epoch+1, train_error, test_error, test_error-train_error))
        
end_time = datetime.datetime.now() # 종료시간을 기록한다
elapsed_time = end_time - start_time # 경과시간을 구한다
print('elapsed_time:',elapsed_time)
print('elapsed_time per epoch:',elapsed_time/epoch_num)

# 하이퍼파라미터 출력
print('input_data_column_cnt:', input_data_column_cnt, end='')
print('output_data_column_cnt:', output_data_column_cnt, end='')
print('seq_length:', seq_length, end='')
print('rnn_cell_hidden_dim:', rnn_cell_hidden_dim, end='')
print('forget_bias:', forget_bias, end='')
print('num_stacked_layers:', num_stacked_layers, end='')
print('keep_prob:', keep_prob, end='')
print('epoch_num:', epoch_num, end='')
print('learning_rate:', learning_rate, end='')
print('train_error:', train_error_summary[-1], end='')
print('test_error:', test_error_summary[-1], end='')
print('min_test_error:', np.min(test_error_summary))

# 결과 그래프 출력
plt.figure(1)
plt.plot(train_error_summary, 'gold')
plt.plot(test_error_summary, 'b')
plt.xlabel('Epoch(x100)')
plt.ylabel('Root Mean Square Error')
 
plt.figure(2)
plt.plot(testY, 'r')
plt.plot(test_predict, 'b')
plt.xlabel('Time Period')
plt.ylabel('FX/KRW')
plt.show()

# sequence length만큼의 가장 최근 데이터를 슬라이싱
recent_data = np.array([x[len(x)-seq_length : ]])
print("recent_data.shape:", recent_data.shape)
print("recent_data:", recent_data)
 
# 내일 환율을 예측
test_predict = sess.run(hypothesis, feed_dict={X: recent_data})
 
print("test_predict", test_predict[0])
test_predict = reverse_min_max_scaling(target_price,test_predict) # target_price 역정규화
print()
print("19.08.23 FX/KRW Prediction", test_predict[0]) # 예측 환율을 출력

